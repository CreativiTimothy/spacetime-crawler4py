"""
This code was mainly generated by AI
and then manually edited to fit assignment specifications.
"""

import re
import logging
from urllib.parse import urlparse, urljoin, urldefrag
from bs4 import BeautifulSoup

from collections import defaultdict
from tokenizer import tokenize
from utils.stopwords import load_stopwords

from analytics_store import load_analytics, save_analytics

from similarity_ngram import (
    make_ngrams,
    select_fingerprints,
    jaccard_similarity,
    FINGERPRINTS,
    NEAR_DUPLICATES
)

# from simhash import simhash, hamming_distance

ALLOWED_DOMAINS = {
    "ics.uci.edu",
    "cs.uci.edu",
    "informatics.uci.edu",
    "stat.uci.edu"
}

STOPWORDS = load_stopwords()

# Logging, can remove later
logger = logging.getLogger("crawler")
if not logger.handlers:
    logger.setLevel(logging.INFO)
    fmt = logging.Formatter("%(asctime)s - %(levelname)s - %(message)s")

    fh = logging.FileHandler("crawl_output.log", encoding="utf-8")
    fh.setFormatter(fmt)

    sh = logging.StreamHandler()
    sh.setFormatter(fmt)

    logger.addHandler(fh)
    logger.addHandler(sh)

# Global analytics
analytics = load_analytics()

UNIQUE_PAGES = set(analytics["unique_pages"])
WORD_COUNTS = defaultdict(int, analytics["word_counts"])
LONGEST_PAGE = tuple(analytics["longest_page"])
SUBDOMAIN_COUNTS = defaultdict(int, analytics["subdomains"])
# FINGERPRINTS = {k: set(v) for k, v in analytics["fingerprints"].items()}
# NEAR_DUPLICATES = analytics["near_duplicates"]

# SIMHASHES = {}          # url -> fingerprint
# NEAR_DUPLICATES = []    # (url1, url2, distance)

def scraper(url, resp):
    """
    Scrape URLs
    """

    if resp.status != 200 or resp.raw_response is None:
        return []

    # Extract links
    links = extract_next_links(url, resp)

    # Extract text for analytics
    try:
        soup = BeautifulSoup(resp.raw_response.content, "html.parser")
        text = soup.get_text(separator=" ")

        """
        Tokenize, Word Analytics, Similarity Detection [Begin]
        """
        tokens = tokenize(text, STOPWORDS)

        # Track unique pages
        normalized_url, _ = urldefrag(url) # Normalize URL by removing fragment
        UNIQUE_PAGES.add(normalized_url)

        # -----------------------------
        # Update Word Counts
        # -----------------------------
        for t in tokens:
            WORD_COUNTS[t] = WORD_COUNTS.get(t, 0) + 1

        # -----------------------------
        # Update Longest Page
        # -----------------------------
        global LONGEST_PAGE
        token_count = len(tokens)

        if token_count > LONGEST_PAGE[1]:
            LONGEST_PAGE = (url, token_count)

        # Update subdomain counts
        parsed = urlparse(url)
        domain = parsed.netloc.lower()
        if domain.endswith(".uci.edu"):
            if domain.startswith("www."):
                domain = domain[4:]
            SUBDOMAIN_COUNTS[domain] += 1

        # Save analytics incrementally
        analytics["unique_pages"] = list(UNIQUE_PAGES)
        analytics["word_counts"] = dict(WORD_COUNTS)
        analytics["longest_page"] = list(LONGEST_PAGE)
        analytics["subdomains"] = dict(SUBDOMAIN_COUNTS)
        # analytics["fingerprints"] = {k: list(v) for k, v in FINGERPRINTS.items()}
        # analytics["near_duplicates"] = NEAR_DUPLICATES

        save_analytics(analytics)
        """
        N-gram Fingerprinting [Begin]
        """
        # -----------------------------
        # N-gram fingerprinting
        # -----------------------------
        ngrams = make_ngrams(tokens)
        fingerprints = select_fingerprints(ngrams)
        FINGERPRINTS[url] = fingerprints

        # Compare with previous pages
        for other_url, other_fp in FINGERPRINTS.items():
            if other_url == url:
                continue

            sim = jaccard_similarity(fingerprints, other_fp)

            # Threshold: textbook suggests ~90% overlap for near-duplicates
            if sim > 0.90:
                NEAR_DUPLICATES.append((url, other_url, sim))
        """
        N-gram Fingerprinting [End]
        """

        """
        SimHash [Begin]
        """
        # # Compute SimHash
        # fp = simhash(tokens)
        # SIMHASHES[url] = fp
        #
        # # Compare with previous pages
        # for other_url, other_fp in SIMHASHES.items():
        #     if other_url == url:
        #         continue
        #     dist = hamming_distance(fp, other_fp)
        #     if dist < 5: # threshold: <5 bits difference
        #         NEAR_DUPLICATES.append((url, other_url, dist))
        """
        SimHash [End]
        """
        """
        Tokenize, Word Analytics, Similarity Detection [End]
        """

    except Exception:
        pass

    # Return only valid links
    cleaned = []
    for link in links:
        link, _ = urldefrag(link)
        if is_valid(link):
            cleaned.append(link)

    return cleaned

def extract_next_links(url, resp):
    """
    Extracts all hyperlinks from the HTML content of the page.
    Returns a list of absolute URLs as strings.
    """
    output_links = []

    try:
        content = resp.raw_response.content
        soup = BeautifulSoup(content, "html.parser")

        for tag in soup.find_all("a", href=True):
            href = tag.get("href")

            # Convert relative â†’ absolute
            abs_url = urljoin(url, href)

            # Remove fragment (#)
            abs_url, _ = urldefrag(abs_url)

            output_links.append(abs_url)

    except Exception as e:
        # If parsing fails, return empty list
        return []

    return output_links

def is_redundant_trap_url(parsed): # Helper function
    path = parsed.path.lower()
    query = parsed.query.lower()
    qs = parse_qs(parsed.query, keep_blank_values=True)

    # -------------------------
    # WICS / NGS avoid
    # -------------------------
    if "wics" in parsed.netloc or "ngs" in parsed.netloc:
        return True

    # -------------------------
    # DokuWiki
    # -------------------------
    if "doku.php" in path:
        if not qs:
            return False
        if set(k.lower() for k in qs.keys()) <= {"id"}:
            return False
        return True

    # -------------------------
    # MediaWiki
    # -------------------------
    if path.endswith("index.php") and "title=" in query:
        allowed = {"title", "oldid"}
        if set(k.lower() for k in qs.keys()) <= allowed:
            return False
        return True

    # -------------------------
    # Trac Wiki
    # -------------------------
    if "/wiki" in path:
        bad = {"version", "format", "action", "from", "precision", "diff"}
        if any(k.lower() in bad for k in qs.keys()):
            return True

    # -------------------------
    # Timeline traps
    # -------------------------
    if "timeline" in path and "from=" in query:
        return True

    # -------------------------
    # UI parameter explosion
    # -------------------------
    generic_bad = (
        "tab=",
        "sort=",
        "order=",
        "filter=",
        "replytocom=",
        "share=",
        "print="
    )

    if any(k in query for k in generic_bad) and query.count("&") >= 3:
        return True

    if len(query) > 200:
        return True

    return False

def is_valid(url):
    """
    Decide whether to crawl this URL.
    Must stay within allowed domains and avoid traps.
    """
    try:
        parsed = urlparse(url)

        if parsed.scheme not in {"http", "https"}:
            return False

        domain = parsed.netloc.lower()

        # strip port if present
        domain = domain.split(":", 1)[0]

        # strip leading www.
        if domain.startswith("www."):
            domain = domain[4:]
        
        # Intranet and gitlab blocks
        if domain == "intranet.ics.uci.edu":
            return False

        if domain == "gitlab.ics.uci.edu" or domain.endswith(".gitlab.ics.uci.edu"):
            return False

        # Domain boundary check
        def allowed_domain(domain: str) -> bool:
            return any(domain == a or domain.endswith("." + a) for a in ALLOWED_DOMAINS)

        if not allowed_domain(domain):
            return False

        if is_redundant_trap_url(parsed):
            return False

        path = parsed.path.lower()
        query = parsed.query.lower()
        q = parse_qs(parsed.query)

        # Avoid file extensions (given by assignment)
        if re.match(
                r".*\.(css|js|bmp|gif|jpe?g|ico"
                + r"|png|tiff?|mid|mp2|mp3|mp4"
                + r"|wav|avi|mov|mpeg|ram|m4v|mkv|ogg|ogv|pdf"
                + r"|ps|eps|tex|ppt|pptx|doc|docx|xls|xlsx|names"
                + r"|data|dat|exe|bz2|tar|msi|bin|7z|psd|dmg|iso"
                + r"|epub|dll|cnf|tgz|sha1"
                + r"|thmx|mso|arff|rtf|jar|csv"
                + r"|rm|smil|wmv|swf|wma|zip|rar|gz)$",
                parsed.path.lower()
        ):
            return False

        # -------------------------
        # Additional trap avoidance
        # -------------------------

        # WordPress libraries
        if "/wp-content/" in path:
            return False

        # Calendar detection
        if any(x in path for x in ["/events", "/calendar"]):
            return False

        if any(x in query for x in ["ical", "outlook"]):
            return False

        # Date archives 
        if re.search(r"^/(19|20)\d{2}/\d{1,2}/page/\d+(/|$)", path):
            return False

        if "paged" in q and any(v.isdigit() and int(v) >= 100 for v in q.get("paged", [])):
            return False

        # Query explosion
        if len(query) > 120 or query.count("&") > 6:
            return False

        # Deep pagination
        if re.search(r"(page|start)=\d{3,}", query):
            return False

        # Deep path 
        if path.count("/") > 10:
            return False

        return True

    except TypeError:
        return False


