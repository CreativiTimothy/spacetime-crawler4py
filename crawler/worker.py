"""
This code was mainly generated by AI
and then manually edited to fit assignment specifications.
"""

from threading import Thread, Lock
from inspect import getsource
from utils.download import download
from utils import get_logger
import scraper
import time

# ============================================================
# GLOBAL POLITENESS CONTROL
# ============================================================
POLITENESS_LOCK = Lock() # Prevents more than 1 thread from accessing.
LAST_REQUEST_TIME = 0.0 # Keep track of last request (download) time.


class Worker(Thread):
    def __init__(self, worker_id, config, frontier):
        self.logger = get_logger(f"Worker-{worker_id}", "Worker")
        self.config = config
        self.frontier = frontier

        # basic check for requests in scraper
        assert {getsource(scraper).find(req) for req in {"from requests import", "import requests"}} == {-1}, \
            "Do not use requests in scraper.py"
        assert {getsource(scraper).find(req) for req in {"from urllib.request import", "import urllib.request"}} == {-1}, \
            "Do not use urllib.request in scraper.py"

        super().__init__(daemon=True)

    def run(self):
        global LAST_REQUEST_TIME

        while True:
            # Thread‑safe frontier access
            tbd_url = self.frontier.get_tbd_url()
            if not tbd_url:
                self.logger.info("Frontier is empty. Stopping Crawler.")
                break

            # ============================================================
            # GLOBAL POLITENESS ENFORCEMENT
            # ============================================================
            with POLITENESS_LOCK:
                now = time.time()
                elapsed = now - LAST_REQUEST_TIME
                if elapsed < self.config.time_delay:
                    time.sleep(self.config.time_delay - elapsed) # Sleep until politeness delay. For example, if it's been 100ms, sleep for 400ms to reach 500ms politeness delay.
                LAST_REQUEST_TIME = time.time() # Update last request time so next thread can check start time of next URL fetch and calculate elapsed time.

                # Only ONE thread downloads at a time
                resp = download(tbd_url, self.config, self.logger)

            self.logger.info(
                f"Downloaded {tbd_url}, status <{resp.status}>, "
                f"using cache {self.config.cache_server}."
            )

            scraped_urls = scraper.scraper(tbd_url, resp)

            # Thread‑safe frontier updates
            for scraped_url in scraped_urls:
                self.frontier.add_url(scraped_url)

            self.frontier.mark_url_complete(tbd_url)
