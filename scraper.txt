"""
This code was mainly generated by AI
and then manually edited to fit assignment specifications.
"""

import re
from urllib.parse import urlparse, urljoin, urldefrag
from bs4 import BeautifulSoup

from collections import defaultdict
from tokenizer import tokenize
from utils.stopwords import load_stopwords

from similarity_ngram import (
    make_ngrams,
    select_fingerprints,
    jaccard_similarity,
    FINGERPRINTS,
    NEAR_DUPLICATES
)

# from simhash import simhash, hamming_distance

ALLOWED_DOMAINS = {
    "ics.uci.edu",
    "cs.uci.edu",
    "informatics.uci.edu",
    "stat.uci.edu"
}

STOPWORDS = load_stopwords()

# Global analytics
WORD_COUNTS = defaultdict(int)
LONGEST_PAGE = ("", 0) # (url, word_count)
SUBDOMAIN_COUNTS = defaultdict(int)

# SIMHASHES = {}          # url -> fingerprint
# NEAR_DUPLICATES = []    # (url1, url2, distance)

def scraper(url, resp):
    """
    Scrape URLs
    """

    if resp.status != 200 or resp.raw_response is None:
        return []

    # Extract links
    links = extract_next_links(url, resp)

    # Extract text for analytics
    try:
        soup = BeautifulSoup(resp.raw_response.content, "html.parser")
        text = soup.get_text(separator=" ")

        """
        Tokenize, Word Analytics, Similarity Detection [Begin]
        """
        tokens = tokenize(text, STOPWORDS)

        # Update word frequencies
        for t in tokens:
            WORD_COUNTS[t] += 1

        # Update longest page
        global LONGEST_PAGE
        if len(tokens) > LONGEST_PAGE[1]:
            LONGEST_PAGE = (url, len(tokens))

        """
        N-gram Fingerprinting [Begin]
        """
        # -----------------------------
        # N-gram fingerprinting
        # -----------------------------
        ngrams = make_ngrams(tokens)
        fingerprints = select_fingerprints(ngrams)
        FINGERPRINTS[url] = fingerprints

        # Compare with previous pages
        for other_url, other_fp in FINGERPRINTS.items():
            if other_url == url:
                continue

        sim = jaccard_similarity(fingerprints, other_fp)

        # Threshold: textbook suggests ~90% overlap for near-duplicates
        if sim > 0.90:
            NEAR_DUPLICATES.append((url, other_url, sim))
        """
        N-gram Fingerprinting [End]
        """

        """
        SimHash [Begin]
        """
        # # Compute SimHash
        # fp = simhash(tokens)
        # SIMHASHES[url] = fp
        #
        # # Compare with previous pages
        # for other_url, other_fp in SIMHASHES.items():
        #     if other_url == url:
        #         continue
        #     dist = hamming_distance(fp, other_fp)
        #     if dist < 5: # threshold: <5 bits difference
        #         NEAR_DUPLICATES.append((url, other_url, dist))
        """
        SimHash [End]
        """
        """
        Tokenize, Word Analytics, Similarity Detection [End]
        """

        # Update subdomain counts
        parsed = urlparse(url)
        domain = parsed.netloc.lower()
        if domain.endswith(".uci.edu"):
            SUBDOMAIN_COUNTS[domain] += 1

    except Exception:
        pass

    # Return only valid links
    cleaned = []
    for link in links:
        link, _ = urldefrag(link)
        if is_valid(link):
            cleaned.append(link)

    return cleaned

def extract_next_links(url, resp):
    """
    Extracts all hyperlinks from the HTML content of the page.
    Returns a list of absolute URLs as strings.
    """
    output_links = []

    try:
        content = resp.raw_response.content
        soup = BeautifulSoup(content, "html.parser")

        for tag in soup.find_all("a", href=True):
            href = tag.get("href")

            # Convert relative â†’ absolute
            abs_url = urljoin(url, href)

            # Remove fragment (#)
            abs_url, _ = urldefrag(abs_url)

            output_links.append(abs_url)

    except Exception as e:
        # If parsing fails, return empty list
        return []

    return output_links


def is_valid(url):
    """
    Decide whether to crawl this URL.
    Must stay within allowed domains and avoid traps.
    """
    try:
        parsed = urlparse(url)

        # Must be HTTP or HTTPS
        if parsed.scheme not in {"http", "https"}:
            return False

        # Must be inside allowed domains
        domain = parsed.netloc.lower()
        if not any(domain.endswith(allowed) for allowed in ALLOWED_DOMAINS):
            return False

        # Avoid file extensions (given by assignment)
        if re.match(
                r".*\.(css|js|bmp|gif|jpe?g|ico"
                + r"|png|tiff?|mid|mp2|mp3|mp4"
                + r"|wav|avi|mov|mpeg|ram|m4v|mkv|ogg|ogv|pdf"
                + r"|ps|eps|tex|ppt|pptx|doc|docx|xls|xlsx|names"
                + r"|data|dat|exe|bz2|tar|msi|bin|7z|psd|dmg|iso"
                + r"|epub|dll|cnf|tgz|sha1"
                + r"|thmx|mso|arff|rtf|jar|csv"
                + r"|rm|smil|wmv|swf|wma|zip|rar|gz)$",
                parsed.path.lower()
        ):
            return False

        # -------------------------
        # Additional trap avoidance
        # -------------------------

        # Avoid calendar traps
        if "calendar" in parsed.path.lower():
            return False

        # Avoid infinite pagination patterns
        if re.search(r"(\?page=\d+|\&page=\d+)", url.lower()):
            return False

        # Avoid repeating directories like /2020/01/01/...
        if parsed.path.count("/") > 10:
            return False

        return True

    except TypeError:
        return False
